{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxikLMwwo5gWEmIDOvjn/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeeeTeeee/Store-Sales-Time-Series-Forecasting/blob/main/Sales_Prediction_nw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2qq77IKjQlZz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from joblib import dump\n",
        "import os\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Loading all datasets\n",
        "holiday = pd.read_csv('/content/holidays_events.csv')\n",
        "oil  = pd.read_csv('/content/oil.csv')\n",
        "stores  = pd.read_csv('/content/stores.csv')\n",
        "test  = pd.read_csv('/content/test.csv')\n",
        "train  = pd.read_csv('/content/train.csv')\n",
        "transaction  = pd.read_csv('/content/transactions.csv')\n",
        "submission  = pd.read_csv('/content/sample_submission.csv')"
      ],
      "metadata": {
        "id": "3IyPwqXgQxaZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "merged_df = pd.merge(train, stores, on='store_nbr', how='left')\n",
        "merged_df = pd.merge(merged_df, transaction, on=['store_nbr', 'date'], how='left')\n",
        "merged_df = pd.merge(merged_df, oil, on=['date'], how='left')\n",
        "data = pd.merge(merged_df, holiday, on=['date'], how='left')\n",
        "\n",
        "# Rename column\n",
        "data.rename(columns={'type_x': 'Store_type'}, inplace=True)\n",
        "data.rename(columns={'type_y': 'Holiday_type'}, inplace=True)\n",
        "\n",
        "# Drop the id column since it's not relevant for modeling\n",
        "data = data.drop('id', axis=1)\n",
        "\n",
        "# Convert the date column to a datetime object\n",
        "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
        "\n",
        "# Set the date column as the index\n",
        "data = data.set_index('date')\n"
      ],
      "metadata": {
        "id": "o8tIo1FaQxWz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Day, Month and Year column from the Date Column\n",
        "data['year'] = data.index.year\n",
        "data['month'] =data.index.month\n",
        "data['day']=data.index.day"
      ],
      "metadata": {
        "id": "vHc-aUaDQxO8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the new super grouping of product family on the actual family attribute.\n",
        "\n",
        "data['family'] = data['family'].replace({\n",
        "'AUTOMOTIVE': 'Others',\n",
        "'BABY CARE': 'Personal Care',\n",
        "'BEAUTY': 'Personal Care',\n",
        "'BEVERAGES': 'Beverages',\n",
        "'BOOKS': 'Others',\n",
        "'BREAD/BAKERY': 'Food',\n",
        "'CELEBRATION': 'Food',\n",
        "'CLEANING': 'Others',\n",
        "'DAIRY': 'Food',\n",
        "'DELI': 'Food',\n",
        "'EGGS': 'Food',\n",
        "'FROZEN FOODS': 'Food',\n",
        "'GROCERY I': 'Food',\n",
        "'GROCERY II': 'Food',\n",
        "'HARDWARE': 'Others',\n",
        "'HOME AND KITCHEN I': 'Home and Kitchen',\n",
        "'HOME AND KITCHEN II': 'Home and Kitchen',\n",
        "'HOME APPLIANCES': 'Home and Kitchen',\n",
        "'HOME CARE': 'Home and Kitchen',\n",
        "'LADIESWEAR': 'Clothing',\n",
        "'LAWN AND GARDEN': 'Others',\n",
        "'LINGERIE': 'Clothing',\n",
        "'LIQUOR,WINE,BEER': 'Beverages',\n",
        "'MAGAZINES': 'Others',\n",
        "'MEATS': 'Food',\n",
        "'PERSONAL CARE': 'Personal Care',\n",
        "'PET SUPPLIES': 'Others',\n",
        "'PLAYERS AND ELECTRONICS': 'Others',\n",
        "'POULTRY': 'Food',\n",
        "'PREPARED FOODS': 'Food',\n",
        "'PRODUCE': 'Food',\n",
        "'SCHOOL AND OFFICE SUPPLIES': 'Others',\n",
        "'SEAFOOD': 'Food'\n",
        "})\n",
        "\n",
        "data['Holiday_type'] = np.where(data['Holiday_type'].isin(['Holiday',\n",
        "                                                                     'Additional', 'Event', 'Transfer', 'Bridge']),\n",
        "                                                                                                                'Holiday', 'Workday')\n",
        "data = data.drop(['locale', 'locale_name', 'description', 'state', 'transferred'], axis=1)"
      ],
      "metadata": {
        "id": "cGPSFBTsUPf4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numeric and non-numeric columns\n",
        "num_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "# Creating imputer variables\n",
        "numerical_imputer = SimpleImputer(strategy = \"mean\")\n",
        "categorical_imputer = SimpleImputer(strategy = \"most_frequent\")\n",
        "\n",
        "\n",
        "# Define the column transformer\n",
        "categorical_features = cat_cols\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', categories='auto', sparse=False))\n",
        "])\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])"
      ],
      "metadata": {
        "id": "HAkB9pcEUPk9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resample numeric columns by mean and categorical columns by mode\n",
        "resampled = data.resample('D').agg({**{col: 'mean' for col in num_cols}, **{col: (lambda x: x.mode()[0] if not x.mode().empty else np.nan) for col in cat_cols}}).reset_index()\n",
        "\n",
        "resampled = resampled.drop('date', axis=1)\n"
      ],
      "metadata": {
        "id": "3QIogDMbUPnq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling missing values in numerical features of training set\n",
        "resampled[num_cols] = numerical_imputer.fit_transform(resampled[num_cols])\n",
        "\n",
        "resampled[cat_cols] = categorical_imputer.fit_transform(resampled[cat_cols])"
      ],
      "metadata": {
        "id": "H0dd18VLUPqb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of rows in the data\n",
        "n_rows = resampled.shape[0]"
      ],
      "metadata": {
        "id": "gLeYFKBjS9vM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the split point\n",
        "split_point = int(n_rows * 0.90)"
      ],
      "metadata": {
        "id": "HSdLtsAeS9rq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the first 85% of the rows as the training data\n",
        "X_train = resampled.iloc[:split_point]\n",
        "y_train = X_train['sales']\n",
        "X_train = X_train.drop('sales', axis=1)"
      ],
      "metadata": {
        "id": "hl1xNJ8HS9oG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the remaining 15% of the rows as the validation data\n",
        "X_eval = resampled.iloc[split_point:]\n",
        "y_eval = X_eval['sales']\n",
        "X_eval = X_eval.drop('sales', axis=1)"
      ],
      "metadata": {
        "id": "HzwkCokkS9lB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # remove 'sales' from num_cols\n",
        "num_cols.remove('sales')"
      ],
      "metadata": {
        "id": "liIGyPwoVPGB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_cat = X_train[cat_cols].copy()\n",
        "X_train_num = X_train[num_cols].copy()\n"
      ],
      "metadata": {
        "id": "Mgqo7HkuVPB_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_eval_cat = X_eval[cat_cols].copy()\n",
        "X_eval_num = X_eval[num_cols].copy()\n"
      ],
      "metadata": {
        "id": "uGpw-1LZVO-v"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the Imputer\n",
        "X_train_cat_imputed = categorical_imputer.fit_transform(X_train_cat)\n",
        "X_train_num_imputed = numerical_imputer.fit_transform(X_train_num)\n",
        "\n",
        "X_eval_cat_imputed = categorical_imputer.fit_transform(X_eval_cat)\n",
        "X_eval_num_imputed = numerical_imputer.fit_transform(X_eval_num)\n",
        "\n",
        "\n",
        "encoder=OneHotEncoder(handle_unknown='ignore')"
      ],
      "metadata": {
        "id": "B8Ay6copVO7c"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the xtrain categories and converting to a dataframe\n",
        "X_train_cat_encoded = encoder.fit(X_train_cat_imputed)\n",
        "X_train_cat_encoded = pd.DataFrame(encoder.transform(X_train_cat_imputed).toarray(),\n",
        "                                   columns=encoder.get_feature_names_out(cat_cols))"
      ],
      "metadata": {
        "id": "7D1Jy1ljVO4d"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the xeval categories and converting to a dataframe\n",
        "X_eval_cat_encoded = encoder.fit(X_eval_cat_imputed)\n",
        "X_eval_cat_encoded = pd.DataFrame(encoder.transform(X_eval_cat_imputed).toarray(),\n",
        "                                   columns=encoder.get_feature_names_out(cat_cols))\n"
      ],
      "metadata": {
        "id": "yfaPJAIQVO1Q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler= StandardScaler()\n",
        "\n",
        "X_train_num_scaled = scaler.fit_transform(X_train_num_imputed)\n",
        "X_train_num_sc = pd.DataFrame(X_train_num_scaled, columns = num_cols)\n",
        "\n",
        "X_eval_num_scaled = scaler.fit_transform(X_eval_num_imputed)\n",
        "X_eval_num_sc = pd.DataFrame(X_eval_num_scaled, columns = num_cols)\n",
        "\n",
        "X_train_df = pd.concat([X_train_num_sc,X_train_cat_encoded], axis =1)\n",
        "X_eval_df = pd.concat([X_eval_num_sc,X_eval_cat_encoded], axis =1)\n"
      ],
      "metadata": {
        "id": "TC4AYM9WVn2Y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary of models to fit\n",
        "models = {\n",
        "    'Random Forest Regressor': RandomForestRegressor(),\n",
        "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
        "    'Gradient Boosting Regressor': GradientBoostingRegressor()\n",
        "}"
      ],
      "metadata": {
        "id": "sG4DGpTyVny2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the models and fit each one to the training data\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_df, y_train)"
      ],
      "metadata": {
        "id": "igUG9gXBVnvK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate each model using cross-validation\n",
        "rmsle_scores = {}\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X_train_df, y_train, cv=50, scoring='neg_mean_squared_log_error')\n",
        "    rmsle_scores[name] = np.sqrt(-scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-uN52wOVnpG",
        "outputId": "282bd228-2b13-480d-affd-54f009bf2fca"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\", line 525, in mean_squared_log_error\n",
            "    raise ValueError(\n",
            "ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the RMSLE scores for each model\n",
        "for name, score in rmsle_scores.items():\n",
        "    print(f'{name}: {score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L6X7SpXVnk7",
        "outputId": "72e2b03a-6b99-49d0-b349-8ec78f6576e9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor: 0.23409874413079693\n",
            "Decision Tree Regressor: 0.3461313766985483\n",
            "Gradient Boosting Regressor: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the model with the lowest RMSLE score\n",
        "best_model_name = min(rmsle_scores, key=rmsle_scores.get)\n",
        "best_model = models[best_model_name]\n",
        "print(f'Best model: {best_model_name}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vWFX7x1V-Ci",
        "outputId": "f3f19b52-214f-4d1f-a8f6-08f681d83ca1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Random Forest Regressor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the destination path to the \"export\" directory\n",
        "destination = \".\"\n",
        "\n",
        "# create a dictionary to store the objects and their filenames\n",
        "models = {\"numerical_imputer\": numerical_imputer,\n",
        "          \"categorical_imputer\": categorical_imputer,\n",
        "          \"encoder\": encoder,\n",
        "          \"scaler\": scaler,\n",
        "          \"Final_model\": best_model}\n",
        "\n",
        "# loop through the models and save them using joblib.dump()\n",
        "for name, model in models.items():\n",
        "    dump(model, os.path.join(destination, f\"{name}.joblib\"))"
      ],
      "metadata": {
        "id": "1SGKYyNtV9-7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "2m1nuCrUV97h"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pipreqs . --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCrh0WZdV93O",
        "outputId": "1fb898d6-c01f-4ef7-b8c2-854f2db0044e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Successfully saved requirements file in ./requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('resampledCmplete.csv', index=False)"
      ],
      "metadata": {
        "id": "0l-5EKYRW8QU"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}